{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'austen-emma.txt',\n",
       " u'austen-persuasion.txt',\n",
       " u'austen-sense.txt',\n",
       " u'bible-kjv.txt',\n",
       " u'blake-poems.txt',\n",
       " u'bryant-stories.txt',\n",
       " u'burgess-busterbrown.txt',\n",
       " u'carroll-alice.txt',\n",
       " u'chesterton-ball.txt',\n",
       " u'chesterton-brown.txt',\n",
       " u'chesterton-thursday.txt',\n",
       " u'edgeworth-parents.txt',\n",
       " u'melville-moby_dick.txt',\n",
       " u'milton-paradise.txt',\n",
       " u'shakespeare-caesar.txt',\n",
       " u'shakespeare-hamlet.txt',\n",
       " u'shakespeare-macbeth.txt',\n",
       " u'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package nltk:\n",
      "\n",
      "NAME\n",
      "    nltk\n",
      "\n",
      "FILE\n",
      "    c:\\users\\admin\\anaconda3\\envs\\two\\lib\\site-packages\\nltk\\__init__.py\n",
      "\n",
      "DESCRIPTION\n",
      "    The Natural Language Toolkit (NLTK) is an open source Python library\n",
      "    for Natural Language Processing.  A free online book is available.\n",
      "    (If you use the library for academic research, please cite the book.)\n",
      "    \n",
      "    Steven Bird, Ewan Klein, and Edward Loper (2009).\n",
      "    Natural Language Processing with Python.  O'Reilly Media Inc.\n",
      "    http://nltk.org/book\n",
      "    \n",
      "    @version: 3.2.2\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    app (package)\n",
      "    book\n",
      "    ccg (package)\n",
      "    chat (package)\n",
      "    chunk (package)\n",
      "    classify (package)\n",
      "    cluster (package)\n",
      "    collections\n",
      "    collocations\n",
      "    compat\n",
      "    corpus (package)\n",
      "    data\n",
      "    decorators\n",
      "    downloader\n",
      "    draw (package)\n",
      "    featstruct\n",
      "    grammar\n",
      "    help\n",
      "    inference (package)\n",
      "    internals\n",
      "    jsontags\n",
      "    lazyimport\n",
      "    metrics (package)\n",
      "    misc (package)\n",
      "    parse (package)\n",
      "    probability\n",
      "    sem (package)\n",
      "    sentiment (package)\n",
      "    stem (package)\n",
      "    tag (package)\n",
      "    tbl (package)\n",
      "    test (package)\n",
      "    text\n",
      "    tgrep\n",
      "    tokenize (package)\n",
      "    toolbox\n",
      "    translate (package)\n",
      "    tree\n",
      "    treeprettyprinter\n",
      "    treetransforms\n",
      "    twitter (package)\n",
      "    util\n",
      "    wsd\n",
      "\n",
      "SUBMODULES\n",
      "    agreement\n",
      "    aline\n",
      "    api\n",
      "    association\n",
      "    bleu_score\n",
      "    bllip\n",
      "    boxer\n",
      "    brill\n",
      "    brill_trainer\n",
      "    casual\n",
      "    chart\n",
      "    confusionmatrix\n",
      "    crf\n",
      "    decisiontree\n",
      "    dependencygraph\n",
      "    discourse\n",
      "    distance\n",
      "    drt\n",
      "    earleychart\n",
      "    evaluate\n",
      "    featurechart\n",
      "    glue\n",
      "    hmm\n",
      "    hunpos\n",
      "    ibm1\n",
      "    ibm2\n",
      "    ibm3\n",
      "    ibm4\n",
      "    ibm5\n",
      "    ibm_model\n",
      "    isri\n",
      "    lancaster\n",
      "    lfg\n",
      "    linearlogic\n",
      "    logic\n",
      "    mace\n",
      "    malt\n",
      "    mapping\n",
      "    maxent\n",
      "    megam\n",
      "    mwe\n",
      "    naivebayes\n",
      "    nltk.app\n",
      "    nltk.chat\n",
      "    nltk.corpus\n",
      "    nltk.toolbox\n",
      "    nonprojectivedependencyparser\n",
      "    paice\n",
      "    pchart\n",
      "    perceptron\n",
      "    porter\n",
      "    positivenaivebayes\n",
      "    projectivedependencyparser\n",
      "    prover9\n",
      "    punkt\n",
      "    recursivedescent\n",
      "    regexp\n",
      "    relextract\n",
      "    repp\n",
      "    resolution\n",
      "    ribes_score\n",
      "    rslp\n",
      "    rte_classify\n",
      "    scikitlearn\n",
      "    scores\n",
      "    segmentation\n",
      "    senna\n",
      "    sequential\n",
      "    sexpr\n",
      "    shiftreduce\n",
      "    simple\n",
      "    snowball\n",
      "    spearman\n",
      "    stack_decoder\n",
      "    stanford\n",
      "    stanford_segmenter\n",
      "    tableau\n",
      "    tadm\n",
      "    textcat\n",
      "    texttiling\n",
      "    tnt\n",
      "    toktok\n",
      "    transitionparser\n",
      "    treebank\n",
      "    viterbi\n",
      "    weka\n",
      "    wordnet\n",
      "\n",
      "FUNCTIONS\n",
      "    demo()\n",
      "        # override any accidentally imported demo\n",
      "\n",
      "DATA\n",
      "    RUS_PICKLE = 'taggers/averaged_perceptron_tagger_ru/averaged_perceptro...\n",
      "    SLASH = *slash*\n",
      "    TYPE = *type*\n",
      "    __author__ = 'Steven Bird, Edward Loper, Ewan Klein'\n",
      "    __author_email__ = 'stevenbird1@gmail.com'\n",
      "    __classifiers__ = ['Development Status :: 5 - Production/Stable', 'Int...\n",
      "    __copyright__ = 'Copyright (C) 2001-2017 NLTK Project.\\n\\nDistribut......\n",
      "    __keywords__ = ['NLP', 'CL', 'natural language processing', 'computati...\n",
      "    __license__ = 'Apache License, Version 2.0'\n",
      "    __longdescr__ = 'The Natural Language Toolkit (NLTK) is a Python ... p...\n",
      "    __maintainer__ = 'Steven Bird, Edward Loper, Ewan Klein'\n",
      "    __maintainer_email__ = 'stevenbird1@gmail.com'\n",
      "    __url__ = 'http://nltk.org/'\n",
      "    __version__ = '3.2.2'\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    class_types = (<type 'type'>, <type 'classobj'>)\n",
      "    corpus = <LazyModule 'nltk.corpus'>\n",
      "    infile = <closed file 'C:\\\\Users\\\\Admin\\\\Anaconda3\\\\envs\\\\two\\\\lib\\\\si...\n",
      "    json_tags = {'!nltk.tag.BrillTagger': <class 'nltk.tag.brill.BrillTagg...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "    string_types = (<type 'basestring'>,)\n",
      "    tkinter = <nltk.compat.TkinterPackage object>\n",
      "    toolbox = <LazyModule 'nltk.toolbox'>\n",
      "    version_file = r'C:\\Users\\Admin\\Anaconda3\\envs\\two\\lib\\site-packages\\n...\n",
      "    version_info = sys.version_info(major=2, minor=7, micro=13, releaselev...\n",
      "\n",
      "VERSION\n",
      "    3.2.2\n",
      "\n",
      "AUTHOR\n",
      "    Steven Bird, Edward Loper, Ewan Klein\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on instance of LazyModule in nltk:\n",
      "\n",
      "nltk.corpus = class LazyModule\n",
      " |  Lazy module class.\n",
      " |  \n",
      " |  Lazy modules are imported into the given namespaces whenever a\n",
      " |  non-special attribute (there are some attributes like __doc__\n",
      " |  that class instances handle without calling __getattr__) is\n",
      " |  requested. The module is then registered under the given name\n",
      " |  in locals usually replacing the import wrapper instance. The\n",
      " |  import itself is done using globals as global namespace.\n",
      " |  \n",
      " |  Example of creating a lazy load module:\n",
      " |  \n",
      " |  ISO = LazyModule('ISO',locals(),globals())\n",
      " |  \n",
      " |  Later, requesting an attribute from ISO will load the module\n",
      " |  automatically into the locals() namespace, overriding the\n",
      " |  LazyModule instance:\n",
      " |  \n",
      " |  t = ISO.Week(1998,1,1)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |      Import the module on demand and get the attribute.\n",
      " |  \n",
      " |  __init__(self, name, locals, globals=None)\n",
      " |      Create a LazyModule instance wrapping module name.\n",
      " |      \n",
      " |      The module will later on be registered in locals under the\n",
      " |      given module name.\n",
      " |      \n",
      " |      globals is optional and defaults to locals.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Import the module on demand and set the attribute.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'austen-emma.txt',\n",
       " u'austen-persuasion.txt',\n",
       " u'austen-sense.txt',\n",
       " u'bible-kjv.txt',\n",
       " u'blake-poems.txt',\n",
       " u'bryant-stories.txt',\n",
       " u'burgess-busterbrown.txt',\n",
       " u'carroll-alice.txt',\n",
       " u'chesterton-ball.txt',\n",
       " u'chesterton-brown.txt',\n",
       " u'chesterton-thursday.txt',\n",
       " u'edgeworth-parents.txt',\n",
       " u'melville-moby_dick.txt',\n",
       " u'milton-paradise.txt',\n",
       " u'shakespeare-caesar.txt',\n",
       " u'shakespeare-hamlet.txt',\n",
       " u'shakespeare-macbeth.txt',\n",
       " u'whitman-leaves.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = nltk.corpus.gutenberg.words(\"melville-moby_dick.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'[', u'Moby', u'Dick', u'by', u'Herman', u'Melville', u'1851', u']']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "906"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.count('whale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.util.StreamBackedCorpusView"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260819"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'StreamBackedCorpusView'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-9ca56b0651ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhelp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamBackedCorpusView\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'StreamBackedCorpusView'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "help(nltk.corpus.reader.util.StreamBackedCorpusView)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "md_set = set(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19317"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.502044830977896"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md)/len(md_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "md_sents = nltk.corpus.gutenberg.sents(\"melville-moby_dick.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'[', u'Moby', u'Dick', u'by', u'Herman', u'Melville', u'1851', u']'], [u'ETYMOLOGY', u'.'], ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10059"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.928919375683467"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md)/len(md_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1789-Washington.txt',\n u'1793-Washington.txt',\n u'1797-Adams.txt',\n u'1801-Jefferson.txt',\n u'1805-Jefferson.txt',\n u'1809-Madison.txt',\n u'1813-Madison.txt',\n u'1817-Monroe.txt',\n u'1821-Monroe.txt',\n u'1825-Adams.txt',\n u'1829-Jackson.txt',\n u'1833-Jackson.txt',\n u'1837-VanBuren.txt',\n u'1841-Harrison.txt',\n u'1845-Polk.txt',\n u'1849-Taylor.txt',\n u'1853-Pierce.txt',\n u'1857-Buchanan.txt',\n u'1861-Lincoln.txt',\n u'1865-Lincoln.txt',\n u'1869-Grant.txt',\n u'1873-Grant.txt',\n u'1877-Hayes.txt',\n u'1881-Garfield.txt',\n u'1885-Cleveland.txt',\n u'1889-Harrison.txt',\n u'1893-Cleveland.txt',\n u'1897-McKinley.txt',\n u'1901-McKinley.txt',\n u'1905-Roosevelt.txt',\n u'1909-Taft.txt',\n u'1913-Wilson.txt',\n u'1917-Wilson.txt',\n u'1921-Harding.txt',\n u'1925-Coolidge.txt',\n u'1929-Hoover.txt',\n u'1933-Roosevelt.txt',\n u'1937-Roosevelt.txt',\n u'1941-Roosevelt.txt',\n u'1945-Roosevelt.txt',\n u'1949-Truman.txt',\n u'1953-Eisenhower.txt',\n u'1957-Eisenhower.txt',\n u'1961-Kennedy.txt',\n u'1965-Johnson.txt',\n u'1969-Nixon.txt',\n u'1973-Nixon.txt',\n u'1977-Carter.txt',\n u'1981-Reagan.txt',\n u'1985-Reagan.txt',\n u'1989-Bush.txt',\n u'1993-Clinton.txt',\n u'1997-Clinton.txt',\n u'2001-Bush.txt',\n u'2005-Bush.txt',\n u'2009-Obama.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538 1789-Washington.txt\n147 1793-Washington.txt\n2585 1797-Adams.txt\n1935 1801-Jefferson.txt\n2384 1805-Jefferson.txt\n1265 1809-Madison.txt\n1304 1813-Madison.txt\n3693 1817-Monroe.txt\n4909"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1821-Monroe.txt\n3150 1825-Adams.txt\n1208 1829-Jackson.txt\n1267 1833-Jackson.txt\n4171 1837-VanBuren.txt\n9165 1841-Harrison.txt\n5196 1845-Polk.txt\n1182 1849-Taylor.txt\n3657 1853-Pierce.txt\n3098 1857-Buchanan.txt\n4005 1861-Lincoln.txt\n785 1865-Lincoln.txt\n1239 1869-Grant.txt\n1478 1873-Grant.txt\n2724 1877-Hayes.txt\n3239 1881-Garfield.txt\n1828 1885-Cleveland.txt\n4750 1889-Harrison.txt\n2153 1893-Cleveland.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4371 1897-McKinley.txt\n2450 1901-McKinley.txt\n1091 1905-Roosevelt.txt\n5846 1909-Taft.txt\n1905 1913-Wilson.txt\n1656 1917-Wilson.txt\n3756 1921-Harding.txt\n4442 1925-Coolidge.txt\n3890 1929-Hoover.txt\n2063 1933-Roosevelt.txt\n2019 1937-Roosevelt.txt\n1536 1941-Roosevelt.txt\n637 1945-Roosevelt.txt\n2528 1949-Truman.txt\n2775 1953-Eisenhower.txt\n1917 1957-Eisenhower.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1546 1961-Kennedy.txt\n1715 1965-Johnson.txt\n2425 1969-Nixon.txt\n2028 1973-Nixon.txt\n1380 1977-Carter.txt\n2801 1981-Reagan.txt\n2946 1985-Reagan.txt\n2713 1989-Bush.txt\n1855 1993-Clinton.txt\n2462 1997-Clinton.txt\n1825 2001-Bush.txt\n2376 2005-Bush.txt\n2726 2009-Obama.txt\n"
     ]
    }
   ],
   "source": [
    "for speech in inaugural.fileids():\n",
    "    words_total = len(inaugural.words(speech))\n",
    "    print words_total, speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inaugural' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b6c154abe3dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mspeech\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minaugural\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0msents_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minaugural\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspeech\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents_total\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inaugural' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "for speech in inaugural.fileids():\n",
    "    sents_total = len(inaugural.sents(speech))\n",
    "    print(sents_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}