{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that looking at the most common and least occuring words in Alice in Wonderland did not proved a list that was very informative.  For our first project we will explore a simple way to automatically find important and descriptive words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the most common words in Alice's Adventures in Wonderland has a lot of commonly used words.  These are words we expect to find in almost any collection of text or book in english.  Lets try looking at a second book, also finding the most common words, then look at the diffrence between those two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets load our familar Alice in Wonderland text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alice = nltk.corpus.gutenberg.words(\"carroll-alice.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alice_fd = nltk.FreqDist(alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save the the top 100 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alice_fd_100 = alice_fd.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have these 100 most common words saved.  Lets do the exact same thing for another book.  We can assume they both will have a lot of the same, no descriptive words.  If we can subtract those two lists from one antoher, we should be left with common words that are mostly unique to the book they came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moby = nltk.corpus.gutenberg.words(\"melville-moby_dick.txt\")\n",
    "moby_fd = nltk.FreqDist(moby)\n",
    "moby_fd_100 = moby_fd.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now have the 100 most words from each book, we no longer care exactly how many times each word was seen.  We now want drop the count so we can easily compare two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alice_100  = [word[0] for word in alice_fd_100]\n",
    "moby_100 = [word[0] for word in moby_fd_100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two sets can be subtracted from one another leaving us with the difference.  Alice - Moby will take the 100 most common words in Alice's Adventures in Wonderland and remove from that list any word if it is also in the top 100 most common words in Moby Dick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"!'\",\n",
       " u'can',\n",
       " u'see',\n",
       " u'your',\n",
       " u'Mock',\n",
       " u\",'\",\n",
       " u'little',\n",
       " u'said',\n",
       " u'll',\n",
       " u'*',\n",
       " u'Hatter',\n",
       " u\".'\",\n",
       " u'much',\n",
       " u'way',\n",
       " u':',\n",
       " u't',\n",
       " u'King',\n",
       " u'do',\n",
       " u'again',\n",
       " u'Alice',\n",
       " u'quite',\n",
       " u'Gryphon',\n",
       " u'know',\n",
       " u\"?'\",\n",
       " u'off',\n",
       " u'herself',\n",
       " u'Turtle',\n",
       " u'don',\n",
       " u'did',\n",
       " u'could',\n",
       " u'Queen',\n",
       " u'm',\n",
       " u'thought',\n",
       " u'she',\n",
       " u'went',\n",
       " u'began']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(alice_100) - set(moby_100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we still get some non descriptive words, you can see many key words are startting to emerge that are iconic to the book such as \"Hatter\", \"Alice\", and \"Queen\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn the operation around and do the same thing for Moby Dick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'!\"',\n",
       " u'old',\n",
       " u'over',\n",
       " u'some',\n",
       " u'head',\n",
       " u'been',\n",
       " u'sea',\n",
       " u'ship',\n",
       " u'any',\n",
       " u'boat',\n",
       " u'\"',\n",
       " u'from',\n",
       " u'.\"',\n",
       " u'But',\n",
       " u'than',\n",
       " u'their',\n",
       " u'only',\n",
       " u'other',\n",
       " u'which',\n",
       " u'?',\n",
       " u'more',\n",
       " u'we',\n",
       " u'Ahab',\n",
       " u'though',\n",
       " u'upon',\n",
       " u'who',\n",
       " u'ye',\n",
       " u'long',\n",
       " u'such',\n",
       " u'whale',\n",
       " u'now',\n",
       " u'him',\n",
       " u'man',\n",
       " u'these',\n",
       " u'will',\n",
       " u'are']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(moby_100) - set(alice_100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very elementary approach to a method we will explore later called TF-IDF (Term Frequency, Inverse Document Frequncy), that will provide very accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
